{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    "### Tutorial  \n",
    "* Web Scraping Indeed for Key Data Science Job Skills [link](https://jessesw.com/Data-Science-Skills/)\n",
    "* Percent-Encoding Reserved Characters [link](https://en.wikipedia.org/wiki/Percent-encoding) \n",
    "\n",
    "### Libraries\n",
    "* This tutorial requires a few python libraries\n",
    "  * BeautifulSoup4\n",
    "  * NLTK [[guide]](http://www.nltk.org/install.html)\n",
    "  * Download NLTK data [[guide]](http://www.nltk.org/data.html)\n",
    "  * urllib2 or urllib3   \n",
    "  \n",
    "> ```pip install beautifulsoup4\n",
    "pip install -U nltk```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib # Website connections\n",
    "import requests\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import matplotlib\n",
    "import nltk\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to cleanup text\n",
    "def headers():\n",
    "    i = random.randint(3,5)\n",
    "    j=random.randint(40,53)\n",
    "    x=random.randint(2,13)\n",
    "    url_base = 'https://www.indeed.ca/jobs?q='\n",
    "    headers ={\n",
    "            'User-Agent':'Mozilla/'+str(i)+'.0 (Macintosh; Intel Mac OS X 10.'+str(x)+'; rv:53.0) Gecko/20100101 Firefox/'+str(j)+'.0'\n",
    "                }\n",
    "    return headers\n",
    "\n",
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = requests.get(website, headers = headers()) # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "\n",
    "    soup_obj = BeautifulSoup(site.text, \"html.parser\") # Get the html from the site\n",
    "\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "\n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "\n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z+3]\",\" \", str(text))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "    return text\n",
    "\n",
    "# note the function itself doesn't return anything if you run this cell\n",
    "\n",
    "#fucntion to get individual count of skills\n",
    "def count_individual_skills(item):\n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item)] # List comp\n",
    "\n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "\n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "\n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "\n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "\n",
    "\n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "    return overall_total_skills\n",
    "\n",
    "\n",
    "\n",
    "# The skills_info function prints out a list of skillset keywords ordered by popularity\n",
    "def skills_info(city = None, state = None, job_title = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "\n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "\n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "\n",
    "    #final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "\n",
    "    final_job = str(job_title)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://ca.indeed.com/jobs?q=', final_job, '&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://ca.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "    print(final_site)\n",
    "\n",
    "    base_url = 'http://ca.indeed.com'\n",
    "\n",
    "\n",
    "    try:\n",
    "        html = requests.get(final_site, headers = headers()) # Open up the front page of our search first\n",
    "    except:\n",
    "        print('That city/state combination did not have any jobs. Exiting . . .') # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html.text, 'html.parser') # Get the html from the first page\n",
    "\n",
    "    # Now find out how many jobs there were\n",
    "\n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "\n",
    "    job_numbers = re.findall(r'\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result\n",
    "\n",
    "\n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "\n",
    "    print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "\n",
    "    num_pages = total_num_jobs//10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    for i in range(1,min(num_pages+2,999)): # Loop through all of our search result pages   \n",
    "    #for i in range(1,5):   # Loop through some of our search result pages(testing)\n",
    "        print('Getting page', i)\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "\n",
    "        html_page = requests.get(current_page, headers = headers()) # Get the page\n",
    "\n",
    "        page_obj = BeautifulSoup(html_page.text,'html.parser') # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "\n",
    "        job_URLS = [str(base_url) + str(link.get('href')) for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "        job_URLS1=[]\n",
    "        # job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "        for abc in job_URLS:\n",
    "            if re.findall(r'clk', abc):\n",
    "                row1 = [ abc ]\n",
    "                outfile1 = \"outLinksRef1.csv\"\n",
    "                csvfile1 = open(outfile1, \"a\")\n",
    "                csvwriter1 = csv.writer(csvfile1)\n",
    "                csvwriter1.writerow(row1)\n",
    "                csvfile1.close()\n",
    "                continue\n",
    "            elif re.findall(r'company', abc):\n",
    "                row1a = [ abc ]\n",
    "                outfile1a = \"outLinksUsed.csv\"\n",
    "                csvfile1a = open(outfile1a, \"a\")\n",
    "                csvwriter1a = csv.writer(csvfile1a)\n",
    "                csvwriter1a.writerow(row1a)\n",
    "                csvfile1a.close()\n",
    "                job_URLS1.append(abc)\n",
    "            else:\n",
    "                row2 = [ abc ]\n",
    "                outfile2 = \"outLinksRef2.csv\"\n",
    "                csvfile2 = open(outfile2, \"a\")\n",
    "                csvwriter2 = csv.writer(csvfile2)\n",
    "                csvwriter2.writerow(row2)\n",
    "                csvfile2.close()\n",
    "                continue\n",
    "                #job_URLS1.append(abc)\n",
    "\n",
    "        for j in range(0,len(job_URLS1)):\n",
    "            final_description = text_cleaner(job_URLS1[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "                individual_count=count_individual_skills(final_description)\n",
    "                row3 = [ job_URLS1[j],' '.join(final_description),individual_count ]\n",
    "                row4 = [ job_URLS1[j],' '.join(final_description) ]\n",
    "                outfile3 = \"outputText.csv\"\n",
    "                csvfile3 = open(outfile3, \"a\")\n",
    "                csvwriter3 = csv.writer(csvfile3,delimiter='|')\n",
    "                csvwriter3.writerow(row3)\n",
    "                csvfile3.close()\n",
    "                outfile4 = \"outputTextWithCounter.csv\"\n",
    "                csvfile4 = open(outfile4, \"a\")\n",
    "                csvwriter4 = csv.writer(csvfile4,delimiter='|')\n",
    "                csvwriter4.writerow(row4)\n",
    "                csvfile4.close()\n",
    "            sleep(0) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "    print('Done with collecting the job postings!')    \n",
    "    print('There were' + str(len(job_descriptions)) + 'jobs successfully found.')\n",
    "\n",
    "\n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "\n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "\n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "\n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "\n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "\n",
    "\n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "\n",
    "\n",
    "\n",
    "    final_frame = pd.DataFrame(list(overall_total_skills.items()), columns=['Term','NumPostings']) # Convert these terms to a dataframe \n",
    "    \n",
    "    # Change the values to reflect a percentage of the postings \n",
    "\n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "\n",
    "    # Sort the data for plotting purposes\n",
    "\n",
    "    final_frame.sort_values(by = 'NumPostings', ascending = False, inplace = True)\n",
    "\n",
    "    # Get it ready for a bar plot\n",
    "\n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                            title = 'Percentage of Data Scientist Job Ads on Indeed, ' + city_title)\n",
    "\n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "    #return fig #,final_frame # End of the function\n",
    "    return overall_total_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ca.indeed.com/jobs?q=Data+Scientist&l=Halifax%2C+NS\n",
      "There were 11 jobs found, Halifax\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Done with collecting the job postings!\n",
      "There were8jobs successfully found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>NumPostings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Java</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C++</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scala</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Excel</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Term  NumPostings\n",
       "0       R         25.0\n",
       "1  Python         25.0\n",
       "2    Java         25.0\n",
       "3     C++         25.0\n",
       "4   Scala         25.0\n",
       "5   Excel         25.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Scientist job keywords in Mississauga\n",
    "skills_info(city = 'Halifax', state = 'NS', job_title='Data+Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Engineer job keywords in Mississauga\n",
    "skills_info(city = 'Toronto', state = 'ON', job_title='Data+Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Engineer job keywords in Mississauga\n",
    "skills_info(city = 'Toronto', state = 'ON', job_title='Business+Analyst') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Engineer job keywords in Mississauga\n",
    "toronto_data_analyst = skills_info(city = 'Toronto', state = 'ON', job_title='Data+Analyst') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
